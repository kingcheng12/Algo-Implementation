{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d3615c8",
   "metadata": {},
   "source": [
    "## Random Forest Implementaion from Scratch\n",
    "\n",
    "This is an implementation of random forest algorithm. The project aims to create a runnable random forest that takes input of numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "2bf97d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from collections import Counter\n",
    "from scipy.stats import mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b72a4ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementation of tree node\n",
    "class Node:\n",
    "    '''\n",
    "    Implement a decision tree node\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, feature=None, threshold=None, left=None, \n",
    "                 right=None, gain=None, value=None):\n",
    "        '''\n",
    "        constructor\n",
    "        '''\n",
    "        self.feature = feature # the selected feature on the node\n",
    "        self.threshold = threshold # threshold for the data split\n",
    "        self.left = left # information for left subtree\n",
    "        self.right = right # information for right subtree\n",
    "        self.gain = gain # gain for the split on this node\n",
    "        self.value = value # target value in leaf node (only!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "4157cca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement of classification tree\n",
    "class Tree:\n",
    "    '''\n",
    "    Implement a binary ID3 decision tree\n",
    "    '''\n",
    "    def __init__(self, min_samples_split=5, max_depth=3,\n",
    "                feature_bagging = None):\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.max_depth = max_depth\n",
    "        self.root = None\n",
    "        self.feature_bagging = feature_bagging # None for no feature bagging, else be number of selected feature, must be > 0 and < p\n",
    "        \n",
    "        \n",
    "    def entropy(self, group):\n",
    "        '''\n",
    "        Compute the entropy of group\n",
    "        \n",
    "        Input: \n",
    "            lis[int] group\n",
    "        Output: \n",
    "            float entropy\n",
    "        '''\n",
    "        # count numbers of each class\n",
    "        count = Counter(group)\n",
    "        # compute the percentage of each class\n",
    "        prob = np.array(list(count.values()))/len(group)\n",
    "        \n",
    "        # compute entropy\n",
    "        return -np.sum(prob * np.log2(prob))\n",
    "        \n",
    "        \n",
    "    def info_gain(self, parent, left, right):\n",
    "        '''\n",
    "        Compute information gain on each node\n",
    "        Input:\n",
    "            list parent\n",
    "            list left\n",
    "            list right\n",
    "        Output:\n",
    "            float information gain\n",
    "        '''\n",
    "        left_portion = len(left) / len(parent)\n",
    "        right_portion = len(right) / len(right)\n",
    "        \n",
    "        return self.entropy(parent) - (left_portion * self.entropy(left) +\n",
    "                                      right_portion * self.entropy(right))\n",
    "        \n",
    "\n",
    "        \n",
    "    def best_split(self, X, Y):\n",
    "        '''\n",
    "        Choose the best feature to split based on information gain\n",
    "        Input:\n",
    "            np.array X: shape (n, p)\n",
    "            np.array Y: shape (n,)\n",
    "        Output:\n",
    "            dictionary best_split\n",
    "        '''\n",
    "        \n",
    "        best_split = {}\n",
    "        best_info_gain =  -1\n",
    "        n_row, n_col = X.shape\n",
    "        \n",
    "        if self.feature_bagging:\n",
    "            _feature = np.random.choice(n_col, self.feature_bagging, replace = False)\n",
    "        else:\n",
    "            _feature = range(n_col)\n",
    "\n",
    "        # for every feature in X\n",
    "        for f_idx in _feature:\n",
    "            X_f = X[:, f_idx]\n",
    "            # for unique value of the choosen feature\n",
    "            for threshold in np.unique(X_f):\n",
    "                # take threshold for split the data\n",
    "                # might be optimized by using bins of the unique value\n",
    "                # instead of all of them\n",
    "                # split based on threshold, left mean <= threshold\n",
    "                X_left = X[X_f <= threshold,:]\n",
    "                X_right = X[X_f > threshold,:]\n",
    "                \n",
    "                \n",
    "                if len(X_left) > 0 and len(X_right) > 0:\n",
    "                    Y_left = Y[X_f <= threshold]\n",
    "                    Y_right = Y[X_f > threshold]\n",
    "                    gain = self.info_gain(Y, Y_left, Y_right)\n",
    "                    if gain > best_info_gain:\n",
    "                        best_split = {\n",
    "                            'feature_index': f_idx,\n",
    "                            'threshold': threshold,\n",
    "                            'X_left': X_left,\n",
    "                            'X_right': X_right,\n",
    "                            'Y_left': Y_left,\n",
    "                            'Y_right': Y_right,\n",
    "                            'gain': gain\n",
    "                        }\n",
    "                        best_info_gain = gain\n",
    "        return best_split\n",
    "                    \n",
    "                \n",
    "        \n",
    "    def grow(self, X, Y, depth = 0):\n",
    "        '''\n",
    "        Build the tree\n",
    "        Input:\n",
    "            np.array X: shape (n, p)\n",
    "            np.array Y: shape (n,)\n",
    "            int depth: current depth of a tree\n",
    "        Output:\n",
    "            Node root\n",
    "        '''\n",
    "        n_row, n_col = X.shape\n",
    "        # check for pre-pruning conditions\n",
    "        if n_row >= self.min_samples_split and depth <= self.max_depth:\n",
    "            best = self.best_split(X,Y)\n",
    "            # check if the split is pure\n",
    "            if best['gain'] > 0:\n",
    "                # grow a tree on left and right\n",
    "                left = self.grow(X=best['X_left'],\n",
    "                                Y=best['Y_left'],\n",
    "                                depth = depth+1)\n",
    "                right = self.grow(X=best['X_right'],\n",
    "                                Y=best['Y_right'],\n",
    "                                depth = depth+1)\n",
    "                return Node(feature=best['feature_index'],\n",
    "                           threshold=best['threshold'],\n",
    "                           left=left,\n",
    "                           right=right,\n",
    "                           gain=best['gain'])\n",
    "        # stop/leaf node, return the most common target value\n",
    "        return Node(value=Counter(Y).most_common(1)[0][0])     \n",
    "        \n",
    "    def fit(self, X, Y):\n",
    "        '''\n",
    "        Train the decision tree with data\n",
    "        Input:\n",
    "            np.array X: shape (n, p)\n",
    "            np.array Y: shape (n,)\n",
    "        Output:\n",
    "            None\n",
    "        '''\n",
    "        self.root = self.grow(X,Y)\n",
    "        \n",
    "    def _predict(self, x, tree):\n",
    "        '''\n",
    "        Predict a single sample based on a given tree\n",
    "        Input:\n",
    "            np.array X: shape (1, p)\n",
    "            Tree tree\n",
    "        Output:\n",
    "           float prediction\n",
    "        '''\n",
    "        \n",
    "        if tree.value != None:\n",
    "            return tree.value\n",
    "        feature_value = x[tree.feature]\n",
    "        \n",
    "        # go downwards\n",
    "        if feature_value <= tree.threshold:\n",
    "            return self._predict(x=x, tree=tree.left)\n",
    "        elif feature_value > tree.threshold:\n",
    "            return self._predict(x=x, tree=tree.right)\n",
    "        \n",
    "    def predict(self, X):\n",
    "        '''\n",
    "        Predict a single sample based on a given tree\n",
    "        Input:\n",
    "            np.array X: shape (n, p)\n",
    "        Output:\n",
    "           np.array prediction: shape (n)\n",
    "        '''\n",
    "        return np.apply_along_axis(self._predict, -1, X, self.root)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0bf2d9",
   "metadata": {},
   "source": [
    "## Test classification tree with iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "id": "86b6a698",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "data = load_iris()\n",
    "X = data['data']\n",
    "y = data['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "id": "e62dbe80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((150, 4), (150,))"
      ]
     },
     "execution_count": 352,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "id": "c90992b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = Tree(feature_bagging=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "id": "6f77e342",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "4b067351",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy is 0.9733333333333334\n"
     ]
    }
   ],
   "source": [
    "print('Train accuracy is {}'.format(sum(clf.predict(X) == y)/len(y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1fe1ce8",
   "metadata": {},
   "source": [
    "Looks like we have maken the tree work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "id": "0e5f4645",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement random forest\n",
    "\n",
    "class RandomForest:\n",
    "    '''\n",
    "    Implement random forest algorith based on class Tree\n",
    "    '''\n",
    "    def __init__(self, num_trees=30, min_samples_split=5, \n",
    "                 max_depth=5, feature_bagging = None):\n",
    "        self.num_trees = num_trees\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.max_depth = max_depth\n",
    "        self.trees = [] # collection of generated trees\n",
    "        self.feature_bagging = feature_bagging\n",
    "        \n",
    "    def bootstrap(self, X, Y):\n",
    "        '''\n",
    "        Sample data with replacement\n",
    "        Input:\n",
    "            np.array X: shape (n, p)\n",
    "            np.array Y: shape (n,)\n",
    "        Output:\n",
    "            np.array X: shape (n, p)\n",
    "            np.array Y: shape (n,)\n",
    "        '''\n",
    "        \n",
    "        n_row, n_col = X.shape\n",
    "        sample = np.random.choice(n_row, n_row, replace = True)\n",
    "        return X[sample], Y[sample]\n",
    "    \n",
    "    def fit(self, X, Y):\n",
    "        '''\n",
    "        Train a random forest model based on data\n",
    "        Input:\n",
    "            np.array X: shape (n, p)\n",
    "            np.array Y: shape (n,)\n",
    "        Output:\n",
    "            None\n",
    "        '''\n",
    "        \n",
    "        tree_count = 0\n",
    "        while tree_count < self.num_trees:\n",
    "            try:\n",
    "                clf = Tree(min_samples_split=self.min_samples_split,\n",
    "                          max_depth=self.max_depth,\n",
    "                          feature_bagging=self.feature_bagging)\n",
    "                _X, _Y = self.bootstrap(X, Y)\n",
    "                clf.fit(_X, _Y)\n",
    "                self.trees.append(clf)\n",
    "                tree_count += 1\n",
    "            except Exception as e:\n",
    "                continue\n",
    "                \n",
    "    def predict(self, X):\n",
    "        '''\n",
    "        Predict a single sample based on a given random forest\n",
    "        Input:\n",
    "            np.array X: shape (n, p)\n",
    "        Output:\n",
    "           np.array prediction: shape (n)\n",
    "        '''\n",
    "        y_hat = np.array(list(map(lambda x: x.predict(X), self.trees)))\n",
    "        \n",
    "        pred, _ = mode(y_hat, axis = 0)\n",
    "        \n",
    "        return pred.ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c491c3d2",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "id": "52924b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_iris()\n",
    "X = data['data']\n",
    "y = data['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "id": "299c7f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForest(feature_bagging=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "id": "8a6a2993",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "id": "197226f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy is 0.9866666666666667\n"
     ]
    }
   ],
   "source": [
    "print('Train accuracy is {}'.format(sum(clf.predict(X) == y)/len(y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06341daa",
   "metadata": {},
   "source": [
    "## The next step is to optimize runtime and apply parallelization when growing trees in random forest.\n",
    "\n",
    "I will use `joblib` packages to do parallel processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "id": "6d1d1f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "id": "fe40f326",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parallel(n_jobs=-2)"
      ]
     },
     "execution_count": 368,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Parallel(n_jobs=-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35cac5aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
